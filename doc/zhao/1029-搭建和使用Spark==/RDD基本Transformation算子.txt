
http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.package

1、创建一个RDD，也可以使用List
	val rdd1 = sc.parallelize(List(5,6,7,8,1,2,3,100,30))
	
    每个元素乘以2，再排序
    val rdd2 = rdd1.map(_*2).sortBy(x=>x,true) 升序
	
    过滤出大于20的元素
    val rdd3 = rdd2.filter(_ > 20)
	
	
2、创建一个RDD，包含字符串（字符）
	val rdd4 = sc.parallelize(Array("a b c","d e f","x y z"))
	分词
	val rdd5 = rdd4.flatMap(_.split(" "))
	
3、集合运算、去重
    val rdd6 = sc.parallelize(List(5,6,7,8,1,2,3,100,30))
	val rdd7 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))
	
	并集：union（如果是SQL中的集合运算，对集合是有要求的）
	val rdd8 = rdd6.union(rdd7)
	去重
	rdd8.distinct.collect
	
	交集: intersect（如果是SQL中的集合运算，对集合是有要求的）
	val rdd9 = rdd6.intersection(rdd7)
	

4、分组
   创建两个RDD<Key，Value>
   
   val rdd1 = sc.parallelize(List(("Tom",1000),("Jerry",3000),("Mary",2000)))
   val rdd2 = sc.parallelize(List(("Jerry",1000),("Tom",3000),("Mike",2000)))
   
   先做一个并集
		val rdd3 = rdd1 union rdd2

		scala> rdd3.collect
		res6: Array[(String, Int)] = Array((Tom,1000), (Jerry,3000), (Mary,2000), (Jerry,1000), (Tom,3000), (Mike,2000))

		scala> val rdd4 = rdd3.groupByKey
		rdd4: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[26] at groupByKey at <console>:30

		scala> rdd4.collect
		res7: Array[(String, Iterable[Int])] = Array( (Tom,CompactBuffer(1000, 3000)),
													  (Jerry,CompactBuffer(3000, 1000)), 
													  (Mike,CompactBuffer(2000)), 
													  (Mary,CompactBuffer(2000)))

参考讲义：P54页

练习7：
val rdd1 = sc.parallelize(List(("tom", 1), ("jerry", 3), ("kitty", 2),  ("shuke", 1)))
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 3), ("shuke", 2), ("kitty", 5)))
val rdd3 = rdd1.union(rdd2)
//按key进行聚合
val rdd4 = rdd3.reduceByKey(_ + _)
rdd4.collect
//按value的降序排序
val rdd5 = rdd4.map(t => (t._2, t._1)).sortByKey(false).map(t => (t._2, t._1))
rdd5.collect

提示：交换了key-value的位置，并且交换了两次











