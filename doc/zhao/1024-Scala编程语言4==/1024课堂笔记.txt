一、Scala的高级特性
	（一）泛型
		1、泛型类：定义类的时候，可以带有一个泛型的参数
		
		2、泛型函数:也可以带有泛型的参数
    ------------------------------------------------------------
		
		3、上界和下界：upper bound、lower bound
		               作用：是规定泛型的取值范围
					   
			（*）简单的例子	
					定义一个变量 ： Int x
					规定的x的取值范围：100 <= x <= 200
					                   下界        上界
									   
			（*）是规定泛型的取值范围
					举例：定义一个泛型（类型的变量）：T
					      类的继承关系： A--->B--->C--->D
						  规定T的取值范围： D <:   T    <: B
						  T的取值只能是：B C D
						  
			（*）概念：
					上界：S <:  T 规定了S的类型必须是T的子类或者本身
					下界：U >:  T 规定了U的类型必须是T的父类或者本身
						 
			（*）scala> //再看一个例子

					scala> //定义一个函数：拼加字符串

					scala> def addTwoString[T<:String](x:T,y:T)= {println(x+"***"+y)}
					addTwoString: [T <: String](x: T, y: T)Unit

					scala> addTwoString("Hello ","World")
					Hello ***World

					scala> addTwoString(1,2)  //希望得到: 1***2
					产生错误。1和2是可以转换成字符串的
					   
		4、视图界定：View bound
					 其实是上界和下界的一种扩展。
					 除了可以接收上界和下界规定的类型以外，还可以接收能够通过隐式转换过去的类型
					 用 % 表示
					 
					 def addTwoString[T<%String](x:T,y:T)= {println(x+"***"+y)}
					 （1）可以接收String和String的子类
					 （2）可以接收能够转换成String的其他数据
					 
					 调用 addTwoString(1,2)
					 error: No implicit view available from Int => String.
					 
					 一定定义转换的规则（通过隐式转换函数来实现）：多了一个关键字 implicit
					 implicit def int2String(n:Int):String = {n.toString}

					调用：
					scala> addTwoString(1,2)
					1***2
					
					执行的过程：（1）首先调用int2String，把Int转成String
					            （2）调用addTwoString，拼加字符串

					 
		5、协变和逆变
			概念：
			（1）协变：表示在类型参数的前面加上 + 
			（2）逆变：表示在类型参数的前面加上 -
	
	（二）隐式转换
		1、隐式转换函数：多了一个关键字 implicit
		2、隐式参数：使用implicit申明的函数参数就叫隐式参数
					scala> //定义一个带有隐式参数的函数

					scala> def testParam(implicit name:String) = println("The value is " + name)
					testParam: (implicit name: String)Unit

					scala> //定义一个隐式参数

					scala> implicit val name:String = "AAAAA"
					name: String = AAAAA

					scala> testParam("BBBBB")
					The value is BBBBB

					scala> testParam
					The value is AAAAA	

					参考讲义：有一个复杂点的例子 P34
		
		3、隐式类：在类名前加上implicit
		           作用：一般来说，可以增强类的功能
	

Spark生态圈：Spark Core: 最重要，其中最重要的就是RDD（弹性分布式数据集）
             Spark SQL 
			 Spark Streaming
			 Spark MLLib: 协同过滤、ALS、逻辑回归等等  ---> 实现推荐系统
			 Spark Graphx：图计算
	
	
Spark Core
--------------------------------------------
一、什么是Spark？特点？
		官网：Apache Spark™ is a unified analytics engine for large-scale data processing.
		      类似MapReduce
			  
		特点：快、易用、通用、兼容
		为什么要学习Spark？基于内存，回顾Mapreduce 2.x的Shuffle


二、安装和部署Spark、Spark的HA
	1、Spark的体系架构：主从架构
		http://spark.apache.org/docs/latest/cluster-overview.html

	2、安装和配置Spark：以Standalone模式为例
		注意：Hadoop和Spark的命令脚本有冲突
		核心配置文件：conf/spark-env.sh
	
		（1）准备工作：安装JDK、配置主机名和免密码登录
		（2）伪分布模式：bigdata111
		                 在一台虚拟机上，模拟一个分布式的环境（Master+Worker）
		
				conf/spark-env.sh
					export JAVA_HOME=/root/training/jdk1.8.0_144
					export SPARK_MASTER_HOST=bigdata111
					export SPARK_MASTER_PORT=7077
				
				conf/slave 配置从节点
				    bigdata111
					
			  启动Spark集群: sbin/start-all.sh
			  Web Console：http://ip:8080
		
		（3）全分布模式：三台机器 bigdata112（Master）  
		                          bigdata113（Worker）  bigdata114（Worker）
		
				conf/spark-env.sh
					export JAVA_HOME=/root/training/jdk1.8.0_144
					export SPARK_MASTER_HOST=bigdata112
					export SPARK_MASTER_PORT=7077
				
				conf/slave 配置从节点
				    bigdata111
				
				把安装包复制到从节点上
					scp -r spark-2.1.0-bin-hadoop2.7/ root@bigdata113:/root/training
					scp -r spark-2.1.0-bin-hadoop2.7/ root@bigdata114:/root/training

				
			  在主节点上，启动Spark集群: sbin/start-all.sh
			  Web Console：http://ip:8080

	3、Spark HA有两种方式：（1）基于文件目录的单点恢复
	                       （2）基于ZooKeeper的Standby的Master
						
						
三、执行Spark的任务：两个工具
	1、spark-submit：用于提交Spark的任务（就是一个jar包）
	2、spark-shell： 相当于REPL，命令行工具
	
四、分析Spark的任务流程
	1、分析WordCount数据处理过程
	2、Spark的调用任务的过程
	
	
五、RDD和RDD的特性、RDD的算子（函数、方法）

六、RDD高级算子

七、编程案例



































