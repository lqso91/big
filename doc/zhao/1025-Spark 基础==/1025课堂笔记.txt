Spark Core
--------------------------------------------
二、安装和部署Spark、Spark的HA
	回顾一下HA：
	（*）HDFS、Yarn、HBase、Storm、Spark  主从结构
	（*）单点故障的问题
	（*）解决方案：HA（High Availability）

	3、Spark HA有两种方式：
	（1）基于文件目录的单点恢复
		（*）本质：还是只有一个主节点Master
		           恢复目录  ---> 保存的是集群的状态和任务的信息
				   重新启动Master，会从恢复目录下读取状态信息
				   主要用于开发或测试环境
				   
			配置
			mkdir /root/training/spark-2.1.0-bin-hadoop2.7/recovery
			文件spark-env.sh
			export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/root/training/spark-2.1.0-bin-hadoop2.7/recovery"
			
			测试：执行一个Application：启动spark-shell命令行工具（作为一个独立的Application运行在Spark集群中）
	                       
	（2）基于ZooKeeper的Standby的Master
		（*）复习ZooKeeper：（bigdata112、bigdata113、bigdata114）
				（*）相当于是一个”数据库“
				（*）数据同步、选举功能、分布式锁功能（秒杀）
	
		配置spark-env.sh
		export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=bigdata112:2181,bigdata113:2181,bigdata114:2181 -Dspark.deploy.zookeeper.dir=/spark"
						
三、执行Spark的任务：两个工具
	1、spark-submit：用于提交Spark的任务（就是一个jar包）
		举例：examples/jars/spark-examples_2.11-2.1.0.jar
		应用：蒙特卡罗求PI（圆周率）
		命令：
		bin/spark-submit --master spark://bigdata111:7077 --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.1.0.jar 100
		
		结果 Pi is roughly 3.1413835141383513
	
	
	2、spark-shell： 相当于REPL，命令行工具；作为一个独立的Application运行
		两种运行模式
		（1）本地模式：不需要链接到Spark集群上，在本地（Eclipse）直接运行；用于开发和测试
		               命令: bin/spark-shell
					   日志：Spark context available as 'sc' (master = local[*], app id = local-1540472971277)
		
		（2）集群模式：需要链接到Spark集群上
		               命令: bin/spark-shell --master spark://bigdata111:7077
					   日志：Spark context available as 'sc' (master = spark://bigdata111:7077, app id = app-20181025211312-0001).
	
			在spark-shell中开发程序 WordCount
			(*) 处理本地文件 直接打印结果
				sc.textFile("/root/temp/input/data.txt").flatMap(_.split(" ")).map((_,1))
				  .reduceByKey(_+_).collect
			
			
			(*) 处理HDFS的文件：输出到HDFS
				sc.textFile("hdfs://bigdata111:9000/input/data.txt").flatMap(_.split(" ")).map((_,1))
				  .reduceByKey(_+_).saveAsTextFile("hdfs://bigdata111:9000/output/1025")
	
	
			(*) 单步运行WordCount  ---->  知识点：RDD
			    val rdd1 = sc.textFile("/root/temp/input/data.txt")
				val rdd2 = rdd1.flatMap(_.split(" "))
				val rdd3 = rdd2.map((_,1))   完整: val rdd3 = rdd2.map((word:String)=>(word,1) )
				val rdd4 = rdd3.reduceByKey(_+_) 完整 
				                    val rdd4 = rdd3.reduceByKey((a:Int,b:Int)=> a+b)
				rdd4.collect
	
四、分析Spark的任务流程:在IDE中，开发一个Scala版本的和Java版本的WordCount
	1、分析WordCount数据处理过程（需要结合源码）
	2、Spark的调用任务的过程
		
五、RDD和RDD的特性、RDD的算子（函数、方法）
	1、RDD：弹性分布式数据集
	2、特性：（1）依赖关系：宽依赖、窄依赖
	3、算子（函数）（1）Transformation：会延时加载（计算）
	               （2）Action：触发计算

六、RDD高级算子

七、编程案例









