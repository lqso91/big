

一、使用PigLatin语句来分析数据
	注意：类似Spark RDD的算子（方法、函数）
			Spark的算子两种类型：
			（*）Transformation：不会触发计算
			（*）Action：会触发计算
	
	注意：启动Yarn的history server
			mr-jobhistory-daemon.sh start historyserver
		  http://ip:19888/jobhistory
		  
	1、常用的PigLatin：有些会触发计算、有些不会
		load：加载数据并生产表
		foreach：相当于循环，对bag中的每一条tuple进行处理
		         Spark：RDD.foreach
		filter：过滤
		         Spark：RDD.filter
		group by：分组
		join：连接操作
		union/intersect：集合运算
		dump：将结果打印屏幕
		store：将结果输出到文件
		generate：提取列值
	
	2、举例：数据：员工表、部门表
			7654,MARTIN,SALESMAN,7698,1981/9/28,1250,1400,30
		（*）加载员工数据到员工表
				emp = load '/scott/emp.csv';
				表结构
				grunt> describe emp;
				Schema for emp unknown.
				grunt> 
				
				加载数据到员工表，并指定结构
				emp = load '/scott/emp.csv' as(empno,ename,job,mgr,hiredate,sal,comm,deptno);
			    默认的数据类型：字节数组
				
				加载数据到员工表，并指定结构和数据的类型
				emp = load '/scott/emp.csv' using PigStorage(',') as(empno:int,ename:chararray,job:chararray,mgr:int,hiredate:chararray,sal:int,comm:int,deptno:int);

			创建部门表
			    dept = load '/scott/dept.csv' using PigStorage(',')  as(deptno:int,dname:chararray,loc:chararray);

		（*）查询员工信息：员工号，姓名，薪水
			SQL：select empno,ename,sal from emp;
			PL:  emp1 = foreach emp generate empno,ename,sal;
				
		（*）查询员工信息，按照月薪排序
		    SQL： select * from emp order by sal;
			PL:   emp2 = order emp by sal;
			      dump emp2;
				  
		（*）分组：求每个部门的工资最大值
		    SQL： select deptno,max(sal) from emp group by deptno;
			PL: 第一步：分组
			       emp3 = group emp by deptno;
				   结构
				   emp3: {group: int,
				          emp: {(empno: int,ename: chararray,job: chararray,mgr: int,hiredate: chararray,sal: int,comm: int,deptno: int)}}
						  
				   数据：
					(10,{(7934,MILLER,CLERK,7782,1982/1/23,1300,0,10),
					     (7839,KING,PRESIDENT,-1,1981/11/17,5000,0,10),
						 (7782,CLARK,MANAGER,7839,1981/6/9,2450,0,10)})
						 
					(20,{(7876,ADAMS,CLERK,7788,1987/5/23,1100,0,20),
					     (7788,SCOTT,ANALYST,7566,1987/4/19,3000,0,20),
						 (7369,SMITH,CLERK,7902,1980/12/17,800,0,20),
						 (7566,JONES,MANAGER,7839,1981/4/2,2975,0,20),
						 (7902,FORD,ANALYST,7566,1981/12/3,3000,0,20)})
						 
					(30,{(7844,TURNER,SALESMAN,7698,1981/9/8,1500,0,30),
					     (7499,ALLEN,SALESMAN,7698,1981/2/20,1600,300,30),
						 (7698,BLAKE,MANAGER,7839,1981/5/1,2850,0,30),
						 (7654,MARTIN,SALESMAN,7698,1981/9/28,1250,1400,30),
						 (7521,WARD,SALESMAN,7698,1981/2/22,1250,500,30),
						 (7900,JAMES,CLERK,7698,1981/12/3,950,0,30)})
				 
				第二步：求每个部门的工资最大值
                    emp4 = foreach emp3 generate group,MAX(emp.sal);		
		
		（*）查询10号部门的员工
		     SQL: select * from emp where deptno=10;
			 PL:  emp5 = filter emp by deptno==10;  注意：两个等号
			 
		（*）多表查询
		      查询员工信息：员工姓名、部门名称
			  SQL：select dname,ename from emp,dept where emp.deptno=dept.deptno;
			  PL：emp6 = join dept by deptno,emp by deptno;
			      emp7 = foreach emp6 generate dept::dname,emp::ename;
				  
		（*）执行：WordCount单词计数
				① 加载数据 
				mydata = load '/input/data.txt' as (line:chararray);

				② 将字符串分割成单词 
				words = foreach mydata generate flatten(TOKENIZE(line)) as word;

				③ 对单词进行分组 
				grpd = group words by word; 

				④ 统计每组中单词数量 
				cntd = foreach grpd generate group,COUNT(words); 

				⑤ 打印结果 
				dump cntd;		
				
				注意：PigLatin中，bag具有依赖关系
				      Spark中，RDD也具有依赖关系（宽依赖、窄依赖）
				
二、Pig的自定义函数：三种
	1、自定义的过滤函数
	2、自定义的运算函数
	3、（最麻烦）自定义的加载函数
		（*）默认：load语句加载数据，一行会被加载成一个Tuple
		（*）需要MapReduce的库（jar文件）
	
	需要的jar包：
	/root/training/pig-0.14.0/pig-0.14.0-core-h2.jar
	/root/training/pig-0.14.0/lib
	/root/training/pig-0.14.0/lib/h2
	/root/training/hadoop-2.4.1/share/hadoop/common
	/root/training/hadoop-2.4.1/share/hadoop/common/lib

三、Sqoop
	1、是数据交换工具：RDBMS <----> Sqoop <---> HDFS（HBase、Hive等等）
	2、基于JDBC
	3、执行数据交换的时候，本质是执行的是一个MapReduce
	4、使用Oracle数据库：自带的测试数据
	   提供一个实验环境：安装Oracle
	   C:\oracle\product\10.2.0\db_1\jdbc\lib\ojdbc14.jar
	   
	   注意：如果是Oracle数据库：大写：用户名、表名、列名
	   
	5、Sqoop的命令
		（*）codegen	将关系数据库表映射为一个Java文件、Java class类、以及相关的jar包
				sqoop codegen --connect jdbc:oracle:thin:@192.168.157.163:1521/orcl --username SCOTT --password tiger --table EMP --outdir /root/temp			
		
		（*）create-hive-table	生成与关系数据库表的表结构对应的HIVE表
		        sqoop create-hive-table --connect jdbc:mysql://localhost:3306/test --table username --username root --password 123456 --hive-table test
		
		
		（*）eval	以快速地使用SQL语句对关系数据库进行操作，这可以使得在使用import这种工具进行数据导入的时候，可以预先了解相关的SQL语句是否正确，并能将结果显示在控制台。
			sqoop eval --connect jdbc:oracle:thin:@192.168.157.163:1521/orcl --username SCOTT --password tiger --query 'select * from emp' 	
		
		
		
		（*）export	从hdfs中导数据到关系数据库中
		（*）help	
		
		
		（*）import	将数据库表的数据导入到HDFS中	
			（1）导入EMP表，所有的列
				sqoop import --connect jdbc:oracle:thin:@192.168.157.163:1521/orcl --username SCOTT --password tiger --table EMP --target-dir /sqoop/import/emp1	
				
			（2）导入指定的列:员工号，姓名，薪水
				sqoop import --connect jdbc:oracle:thin:@192.168.157.163:1521/orcl --username SCOTT --password tiger --table EMP --columns EMPNO,ENAME,SAL --target-dir /sqoop/import/emp2
					
			
			（3）导入订单表：大概有92万数据
				sqoop import --connect jdbc:oracle:thin:@192.168.157.163:1521/orcl --username SH --password sh --table SALES --target-dir /sqoop/import/sales -m 1
				
				错误
				ERROR tool.ImportTool: Error during import: No primary key could be found for table SALES. Please specify one with --split-by or perform a sequential import with '-m 1'.
		
		
		
		（*）import-all-tables	将数据库中所有的表的数据导入到HDFS中
		（*）job	用来生成一个sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务。
		（*）list-databases	打印出关系数据库所有的数据库名
		（*）list-tables	打印出关系数据库某一数据库的所有表名
		（*）merge	将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中
		（*）metastore	记录sqoop job的元数据信息
		
		
		（*）version	显示sqoop版本信息	
		     sqoop  version

四、Flume：采集日志






























