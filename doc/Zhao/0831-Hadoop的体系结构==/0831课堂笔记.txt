一、Hadoop的安装与配置
	（一）免密码登录的原理和配置
	    查看上节笔记
	（二）全分布模式：真正用于生产的环境
		1、至少需要3台机器
		2、集群的规划
		3、准备工作
			（*）安装三台Linux、JDK、关闭防火墙
			（*）设置主机名和IP  vi /etc/hosts
					192.168.157.112 bigdata112
					192.168.157.113 bigdata113
					192.168.157.114 bigdata114
			
			（*）配置免密码登录：两两之间的免密码登录
					ssh-copy-id -i .ssh/id_rsa.pub root@bigdata112
					ssh-copy-id -i .ssh/id_rsa.pub root@bigdata113
					ssh-copy-id -i .ssh/id_rsa.pub root@bigdata114
					
			（*）保证集群的时间同步
			        时间同步：
                        yum -y install ntp
                        systemctl enable ntpd
                        systemctl start ntpd
                        ntpdate -u cn.pool.ntp.org
                    时区设置：
                        timedatectl status  # 查看系统时间方面的各种状态
                        timedatectl list-timezones  # 列出所有时区
                        timedatectl set-local-rtc 1 # 将硬件时钟调整为与本地时钟一致, 0 为设置为 UTC 时间
                        timedatectl set-timezone Asia/Shanghai  # 设置系统时区为上海
                        
		4、在主节点上安装（bigdata112）
			（*）解压和设置环境变量
			（*）hadoop-env.sh
					export JAVA_HOME=/opt/jdk1.8.0_144
					
			（*）hdfs-site.xml
					<!--配置数据块的冗余度,默认是3-->
					<!--原则冗余度跟数据节点个数保持一致,最大不要超过3-->
					<property>	
						<name>dfs.replication</name>
						<value>3</value>
					</property>

					<!--是否开启HDFS的权限检查，默认是true-->
					<!--使用默认值，后面会改为false-->
					<!--
					<property>	
						<name>dfs.permissions</name>
						<value>false</value>
					</property>				
                    -->			
					
			（*）core-site.xml
					<!--配置HDFS主节点的地址，就是NameNode的地址-->
					<!--9000是RPC通信的端口-->
					<property>	
						<name>fs.defaultFS</name>
						<value>hdfs://bigdata112:9000</value>
					</property>	

					<!--HDFS数据块和元信息保存在操作系统的目录位置-->
					<!--默认是Linux的tmp目录,一定要修改-->
					<property>	
						<name>hadoop.tmp.dir</name>
						<value>/opt/hadoop-2.7.3/tmp</value>
					</property>			
			
			（*）mapred-site.xml
					<!--MR程序运行容器或者框架-->
					<property>	
						<name>mapreduce.framework.name</name>
						<value>yarn</value>
					</property>				
			
			（*）yarn-site.xml
			        <!--配置Yarn主节点的位置-->
					<property>	
						<name>yarn.resourcemanager.hostname</name>
						<value>bigdata112</value>
					</property>

					<!--NodeManager执行MR任务的方式是Shuffle洗牌-->
					<property>	
						<name>yarn.nodemanager.aux-services</name>
						<value>mapreduce_shuffle</value>
					</property>				
			（*）slaves 配置从节点地址
			      bigdata112
			      bigdata113
				  bigdata114
			
			（*）对namenode进行格式化
			      hdfs namenode -format

		5、把bigdata112上安装好的目录复制到从节点上
			scp -r hadoop-2.7.3/ root@bigdata113:/opt
			scp -r hadoop-2.7.3/ root@bigdata114:/opt
		
		6、在主节点上启动集群
		    HDFS：start-dfs.sh  ---> 使用hdfs时启动
            Yarn: start-yarn.sh ---> 使用mapreduce时启动
            统一的：start-all.sh

            Web Console访问：hdfs: 端口: 50070
                             yarn: 端口：8088
		   
		7、跟伪分布一样，在主节点上执行WordCount
	        hadoop jar /opt/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /wc/input/data.txt /wc/output
	        查看输出：
	        hdfs dfs -ls /wc/output
	        hdfs dfs -cat /wc/output/part-r-00000
	（三）主从结构的单点故障：暂时不解决
	
二、HDFS：数据存储
    （一）目标
        1、存储非常大的文件
            这里非常大指的是几百M、G、或者TB级别。 实际应用中已有很多集群存储的数据达到PB级别。
            根据Hadoop官网，Yahoo！的Hadoop集群约有10万颗CPU，运行在4万个机器节点上。
        2、采用流式的数据访问方式
            HDFS基于这样的一个假设：最有效的数据处理模式是一次写入、多次读取数据集经常从数据源生成或者拷贝一次，然后在其上做很多分析工作
            分析工作经常读取其中的大部分数据，即使不是全部。 因此读取整个数据集所需时间比读取第一条记录的延时更重要。
        3、运行于商业硬件上
            Hadoop不需要特别贵的、reliable的机器，可运行于普通商用机器（可以从多家供应商采购） 商用机器不代表低端机器在集群中（尤其是大的集群），
            节点失败率是比较高的HDFS的目标是确保集群在节点失败的时候不会让用户感觉到明显的中断。

    （二）HDFS不适合的应用类型
        1、低延时的数据访问
            对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能牺牲延时HBase更适合低延时的数据访问。
        2、大量小文件
            文件的元数据（如目录结构，文件block的节点列表，block-node mapping）保存在NameNode的内存中， 整个文件系统的文件数量会受限于NameNode的内存大小。
            经验而言，一个文件/目录/文件块一般占有150字节的元数据内存空间。如果有100万个文件，每个文件占用1个文件块，则需要大约300M的内存。
            因此十亿级别的文件数量在现有商用机器上难以支持。
        3、多方读写，需要任意的文件修改
            HDFS采用追加（append-only）的方式写入数据。不支持文件任意offset的修改。不支持多个写入器（writer）。

	（三）HDFS的体系架构
		1、NameNode：名称节点
			（*）职责：
				（1）是HDFS的主节点、管理员
				（2）接收客户端（命令行、Java程序）的请求：创建目录、上传数据、下载数据、删除数据等等
				（3）管理和维护HDFS的日志和元信息
					（*）日志文件（edits）：记录的是客户端的所有操作，同时体现了HDFS的最新的状态
						 是一个二进制文件
						 位置：$HADOOP_HOME/tmp/dfs/name/current
						 edits_inprogress_0000000000000000107 代表：正在操作的日志文件
						 
						 举例：hdfs dfs -mkdir /aaa
						 HDFS提供了一个日志查看器（edits viewer），把edits文件转成xml格式(默认xml)
						 hdfs oev -i /opt/hadoop-2.7.6/tmp/dfs/name/current/edits_inprogress_0000000000000000440 -o edits.xml

					（*）元信息文件（fsimage）：记录的是数据块的位置信息、数据块的冗余信息
					     没有体现HDFS的最新状态！！！
					     是一个二进制文件
						 位置：$HADOOP_HOME/tmp/dfs/name/current
						 HDFS提供了一个元信息查看器（image viewer），把fsimage文件转为xml(默认hdfsweb)
					     hdfs oiv -i /opt/hadoop-2.7.6/tmp/dfs/name/current/fsimage_0000000000000000439 -p XML

		2、DataNode：数据节点
			（*）职责：按照数据块保存数据库
			            1.x： 64M
						2.x：128M
						
			（*）数据块：表现形式：就是一个文件（blk*******）
					位置：/opt/hadoop-2.7.6/tmp/dfs/data/current/BP-993497267-192.168.254.112-1543805382490/current/finalized/subdir0/subdir0/
					
					举例：上传一个大于128M的文件
					
			（*）设置数据块冗余度原则：一般跟数据节点的个数一样；但是最大不要超过3
			（*）Hadoop 3.x以前，会造成存储空间的极大浪费
			     Hadoop 3.x以后，HDFS纠删码技术，大大的节约存储的空间（节约一半 ）
		
		3、SecondaryNameNode：第二名称节点
			职责：进行日志信息的合并
			
			（*）由于edits文件记录了最新的状态信息，并且随着操作越多，edits就会越大
			（*）把edits中的最新信息写到fsimage中
			（*）edits文件就可以清空
			
			补充点知识：检查点checkpoint
			（*）Spark中的RDD的检查点：容错机制
			（*）Oracle中的检查点：会以最高优先级唤醒数据库的写进程，将脏数据写入硬盘文件

        4、Blocks
                物理磁盘中有块的概念，磁盘的物理Block是磁盘操作最小的单元，读写操作均以Block为最小单元，一般为512 Byte。
            文件系统在物理Block之上抽象了另一层概念，文件系统Block物理磁盘Block的整数倍。通常为几KB。
            Hadoop提供的df、fsck这类运维工具都是在文件系统的Block级别上进行操作。

                HDFS的Block块比一般单机文件系统大得多，默认为128M。HDFS的文件被拆分成block-sized的chunk，
            chunk作为独立单元存储。比Block小的文件不会占用整个Block，只会占据实际大小。例如， 如果一个文件大小为1M，
            则在HDFS中只会占用1M的空间，而不是128M。

            HDFS的Block为什么这么大？
                是为了最小化查找（seek）时间，控制定位文件与传输文件所用的时间比例。假设定位到Block所需的时间为10ms，
            磁盘传输速度为100M/s。如果要将定位到Block所用时间占传输时间的比例控制1%，则Block大小需要约100M。
            但是如果Block设置过大，在MapReduce任务中，Map或者Reduce任务的个数 如果小于集群机器数量，会使得作业运行效率很低。

            Block抽象的好处
                block的拆分使得单个文件大小可以大于整个磁盘的容量，构成文件的Block可以分布在整个集群， 理论上，
            单个文件可以占据集群中所有机器的磁盘。
            Block的抽象也简化了存储系统，对于Block，无需关注其权限，所有者等内容（这些内容都在文件级别上进行控制）。
            Block作为容错和高可用机制中的副本单元，即以Block为单位进行复制。

深入理解HDFS：Hadoop分布式文件系统（该博客内容整理自Hadoop权威指南）
https://blog.csdn.net/bingduanlbd/article/details/51914550