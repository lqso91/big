一、操作HDFS
	1、Web Console：端口 50070
	
	2、命令行：类似Linux的命令
		（*）普通操作命令：hdfs dfs ****（另一种写法：hadoop fs *****）
			-mkdir
			
			查看目录
			-ls
			-ls -R 查看目录和子目录，另一种写法：-lsr
			
			上传数据
			-put
			-copyFromLocal
			-moveFromLocal: 相当于ctrl+X（剪切）
			
			下载数据
			-get
			-copyToLocal
			
			-rm：删除
				参数： -rmr 表示删除目录和子目录
				hdfs dfs -rmr /output
				日志：Deleted /output
				注意：如果启用回收站，观察日志
			
			-getmerge：把某个HDFS的目录下的文件，先合并，再下载
				举例：（students的数据，就是Hive的外部表用的数据）
					[root@bigdata111 ~]# hdfs dfs -mkdir /students
					[root@bigdata111 ~]# vi student01.txt
					[root@bigdata111 ~]# vi student02.txt
					[root@bigdata111 ~]# hdfs dfs -put student0*.txt /students
					[root@bigdata111 ~]# hdfs dfs -ls /students
					Found 2 items
					-rw-r--r--   1 root supergroup         19 2018-09-04 04:46 /students/student01.txt
					-rw-r--r--   1 root supergroup         10 2018-09-04 04:46 /students/student02.txt
					[root@bigdata111 ~]# hdfs dfs -getmerge /students /root/a.txt
					[root@bigdata111 ~]# more a.txt 
					1,Tom,23
					2,Mary,26
					3,Mike,24
			
			-cp
			-mv
			
			-count：统计HDFS目录下目录个数、文件个数、文件总的大小
			[root@bigdata111 ~]# hdfs dfs -count /students
					   1            2                 29 /students
			
			-du: hdfs dfs -du /students
				19  /students/student01.txt
				10  /students/student02.txt
			
			-text、-cat: 查看文本的内容
			balancer: 参考讲义	
	
		（*）HDFS的管理命令：hdfs dfsadmin *****
			[-report [-live] [-dead] [-decommissioning]]
			打印报告：
			hdfs dfsadmin -report
			
			[-safemode <enter | leave | get | wait>]
			管理安全模式的命令
			enter：手动进入安全模式
			leave: 手动离开安全模式
			get: 当前状态
			
			hdfs dfsadmin -safemode get
		
	3、Java API
	    http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/filesystem/index.html

		依赖的jar包：
		$HADOOP_HOME/share/hadoop/common/*.jar
		$HADOOP_HOME/share/hadoop/common/lib/*.jar
		
		$HADOOP_HOME/share/hadoop/hdfs/*.jar
		$HADOOP_HOME/share/hadoop/hdfs/lib/*.jar
		
		MAVEN:
		<properties>
            <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
            <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
            <java.version>1.8</java.version>
            <hadoop.version>2.7.6</hadoop.version>
        </properties>

        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>

		（*）创建目录: HDFS的权限的问题
					<!--是否开启HDFS的权限检查，默认是true-->
					<!--使用默认值，后面会改为false-->
					<!--
					<property>	
						<name>dfs.permissions</name>
						<value>false</value>
					</property>				
                    -->			
		
		（*）上传数据、下载数据
		（*）查看文件的信息
		（*）数据节点信息
		（*）等等

二、HDFS的原理分析（非常重要）
	1、数据上传的过程
	    1）客户端调用DistributedFileSystem的create方法
        2）DistributedFileSystem远程RPC调用Namenode在文件系统的命名空间中创建一个新文件，此时该文件没有关联到任何block。
            这个过程中，Namenode会做很多校验工作，例如是否已经存在同名文件，是否有权限，如果验证通过，
            返回一个FSDataOutputStream对象。 如果验证不通过，抛出异常到客户端。
        3）客户端写入数据的时候，DFSOutputStream分解为packets，并写入到一个数据队列中，该队列由DataStreamer消费。
        4）DateStreamer负责请求Namenode分配新的block存放的数据节点。这些节点存放同一个Block的副本，构成一个管道。
            DataStreamer将packer写入到管道的第一个节点，第一个节点存放好packer之后，转发给下一个节点，
            下一个节点存放 之后继续往下传递。
        5）DFSOutputStream同时维护一个ack queue队列，等待来自datanode确认消息。当管道上的所有datanode都确认之后，
            packer从ack队列中移除。
        6）数据写入完毕，客户端close输出流。将所有的packet刷新到管道中，然后安心等待来自datanode的确认消息。
            全部得到确认之后告知Namenode文件是完整的。 Namenode此时已经知道文件的所有Block信息
            （因为DataStreamer是请求Namenode分配block的），只需等待达到最小副本数要求，然后返回成功信息给客户端。

	2、数据下载的过程
        1）客户端传递一个文件Path给FileSystem的open方法
        2）DFS采用RPC远程获取文件最开始的几个block的datanode地址。Namenode会根据网络拓扑结构决定返回哪些节点
            （前提是节点有block副本），如果客户端本身是Datanode并且节点上刚好有block副本，直接从本地读取。
        3）客户端使用open方法返回的FSDataInputStream对象读取数据（调用read方法）
        4）DFSInputStream（FSDataInputStream实现了该类）连接持有第一个block的、最近的节点，反复调用read方法读取数据
        5）第一个block读取完毕之后，寻找下一个block的最佳datanode，读取数据。如果有必要，
            DFSInputStream会联系Namenode获取下一批Block 的节点信息(存放于内存，不持久化），这些寻址过程对客户端都是不可见的。
        6）数据读取完毕，客户端调用close方法关闭流对象
            在读数据过程中，如果与Datanode的通信发生错误，DFSInputStream对象会尝试从下一个最佳节点读取数据，
            并且记住该失败节点， 后续Block的读取不会再连接该节点。读取一个Block之后，DFSInputStram会进行检验和验证，
            如果Block损坏，尝试从其他节点读取数据，并且将损坏的block汇报给Namenode。
            客户端连接哪个datanode获取数据，是由namenode来指导的，这样可以支持大量并发的客户端请求，
            namenode尽可能将流量均匀分布到整个集群。
            Block的位置信息是存储在namenode的内存中，因此相应位置请求非常高效，不会成为瓶颈。

三、HDFS的高级特性
	1、安全模式 safe mode
		注意：HDFS正常运行的时候，安全模式一定是off（关闭状态）
		      是HDFS的一种自我保护，作用：检查数据块的副本率
			  HDFS处于安全模式，是只读的状态
	
	2、快照：是一种备份
		命令：
			[-allowSnapshot <snapshotDir>]
			[-disallowSnapshot <snapshotDir>]
			
	3、配额：Quota	
		（1）名称配额
			[-setQuota <quota> <dirname>...<dirname>]
			[-clrQuota <dirname>...<dirname>]
		
		（2）空间配额
			[-setSpaceQuota <quota> [-storageType <storagetype>] <dirname>...<dirname>]
			[-clrSpaceQuota [-storageType <storagetype>] <dirname>...<dirname>]

	4、回收站：默认HDFS的回收站禁用	
	

四、HDFS底层的原理
	1、RPC
	2、代理对象







