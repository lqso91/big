
数据分析引擎：Hive

大数据的终极目标：使用SQL语句来处理大数据
1、Hadoop的体系架构中：
	（*）Hive：支持SQL
	（*）Pig： 支持PigLatin
2、Spark的体系架构中：
	（*）Spark SQL：类似Hive
	                支持SQL、支持DSL
					
3、另一个：Impala
=========================================
一、什么是Hive？
	1、Hive是基于HDFS之上的一个数据仓库
		Hive       HDFS
		表         目录
		数据       文件
		分区       目录
		桶         文件
		
	2、Hive基于Hadoop之上的一个数据分析引擎
	   Hive是一个翻译器
		Hive 2.x 以前：SQL -----> Hive  ----> MapReduce
		Hive 2.x 以后：推荐使用Spark作为SQL的执行引擎（只针对Hadoop 3.x以前）
		               推荐《Hive on Spark文档》
	
	3、Hive支持SQL的一个子集（HQL）
	

二、Hive的体系架构

三、安装和配置Hive
	安装模式：
	（*）嵌入模式：不需要使用MySQL，使用Hive自带的Derby数据库存储Hive的元信息
	（*）本地模式、远程模式：都需要MySQL
	
	准备工作：
		1、解压  tar -zxvf apache-hive-2.3.0-bin.tar.gz -C ~/training/
		2、设置环境变量 vi  ~/.bash_profile
			HIVE_HOME=/root/training/apache-hive-2.3.0-bin
			export HIVE_HOME

			PATH=$HIVE_HOME/bin:$PATH
			export PATH
			
	核心的配置文件  conf/hive-site.xml
	
	1、安装Hive的嵌入模式
		特点：（1）使用自带的Derby
		      （2）只支持一个连接
			  （3）用于开发和测试
		创建hive-site.xml
			<?xml version="1.0" encoding="UTF-8" standalone="no"?>
			<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
			<configuration>
				<property>
					<name>javax.jdo.option.ConnectionURL</name>
					<value>jdbc:derby:;databaseName=metastore_db;create=true</value>
				</property>
				
				<property>
					<name>javax.jdo.option.ConnectionDriverName</name>
					<value>org.apache.derby.jdbc.EmbeddedDriver</value>
				</property>	
				
				<property>
					<name>hive.metastore.local</name>
					<value>true</value>
				</property>	

				<property>
					<name>hive.metastore.warehouse.dir</name>
					<value>file:///root/training/apache-hive-2.3.0-bin/warehouse</value>
				</property>			
			</configuration>

		初始化MetaStore：
			schematool -dbType derby -initSchema
			
		  日志：
		  Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.


	2、安装配置MySQL数据库
		在虚拟机上安装MySQL：
			yum remove mysql-libs 
			rpm -ivh mysql-community-common-5.7.18-1.el7.x86_64.rpm
			rpm -ivh mysql-community-libs-5.7.18-1.el7.x86_64.rpm
			rpm -ivh mysql-community-client-5.7.18-1.el7.x86_64.rpm
			rpm -ivh mysql-community-server-5.7.18-1.el7.x86_64.rpm
			rpm -ivh mysql-community-devel-5.7.18-1.el7.x86_64.rpm  （可选，但还是装上，后面装HUE的时候会用到。）
					
		  启动MySQL：service mysqld start
		  或者：systemctl start mysqld.service

        ----------------使用MariaDB----------------
            yum install mariadb* -y

            systemctl start mariadb
            systemctl enable mariadb

            使用脚本初始化密码:
            mysql_secure_installation
            修改密码:
            UPDATE user SET password=password('Abc12345') WHERE user='root' and host='localhost';
            flush privileges;
            或
            SET password for 'root'@'localhost'=password('Abc12345');

        -------------------------------------------

		  查看root用户的密码：cat /var/log/mysqld.log | grep password
		  登录后修改密码：alter user 'root'@'localhost' identified by 'Welcome_1';

		MySQL数据库的配置：
		创建一个新的数据库：create database hive;
		创建一个新的用户：
				create user 'hiveowner'@'%' identified by 'Welcome_1';
		
			给该用户授权
					   grant all on hive.* TO 'hiveowner'@'%'; 
					   grant all on hive.* TO 'hiveowner'@'localhost' identified by 'Welcome_1';		  
		  
			免费工具：http://www.mysqlfront.de/
		  
	3、本地模式、远程模式
		创建hive-site.xml
			<?xml version="1.0" encoding="UTF-8" standalone="no"?>
			<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
			<configuration>
				<property>
					<name>javax.jdo.option.ConnectionURL</name>
					<value>jdbc:mysql://localhost:3306/hive?useSSL=false</value>
				</property>				
				<property>
					<name>javax.jdo.option.ConnectionDriverName</name>
					<value>com.mysql.jdbc.Driver</value>
				</property>	
				<property>
					<name>javax.jdo.option.ConnectionUserName</name>
					<value>hiveowner</value>
				</property>	
				<property>
					<name>javax.jdo.option.ConnectionPassword</name>
					<value>Welcome_1</value>
				</property>					
			</configuration>

        初始化MetaStore：
            schematool -dbType mysql -initSchema

四、（最重要）Hive的数据模型
	1、内部表：类似MySQL、Oracle中表  
		默认的列的分隔符是：tab键
		
		举例：创建一个员工表 emp表
		      数据 7654,MARTIN,SALESMAN,7698,1981/9/28,1250,1400,30

        create table emp(empno int,ename string,job string,mgr int,hiredate string,
        sal int,comm int,deptno int);
			  
			  创建一个员工表 emp1表，并且指定分隔符是：逗号(用于导入csv文件)
		create table emp1(empno int, ename string, job string, mgr int, hiredate string,
		sal int, comm int, deptno int)row format delimited fields terminated by ',';

			  DROP TABLE emp;
			  ALTER TABLE emp1 RENAME TO emp;

		导入数据：
			insert语句
			load语句：相当于ctrl+x
				（1）导入HDFS的数据   load data inpath '/scott/emp.csv' into table emp;
				（2）导入本地Linux的数据：load data local inpath '/root/temp/emp.csv' into table emp1;
		
		客户端的静默模式：不打印日志
	
	2、分区表：提高性能
		举例：创建一个分区表，根据部门号deptno
			  create table emp_part
			  (empno int,
			   ename string,
			   job   string,
			   mgr   int,
			   hiredate string,
			   sal   int,
			   comm  int
			  )partitioned by (deptno int)
			  row format delimited fields terminated by ',';

			往分区表中插入数据：子查询 
insert into table emp_part partition(deptno=10) select empno,ename,job,mgr,hiredate,sal,comm from emp1 where deptno=10;
insert into table emp_part partition(deptno=20) select empno,ename,job,mgr,hiredate,sal,comm from emp1 where deptno=20;
insert into table emp_part partition(deptno=30) select empno,ename,job,mgr,hiredate,sal,comm from emp1 where deptno=30;

		（*）补充：如何判断提高了性能？ ----> 重点讲：关系型数据库（Oracle）、Hive类似方式
		                                ----> SQL执行计划

	3、外部表
										
	4、桶表：类似Hash分区
	
	5、视图：View

五、执行Hive的查询：执行SQL（HQL）
	注意：HQL是SQL的一个子集
	
六、使用JDBC查询Hive

七、Hive的自定义函数：本质是Java程序

