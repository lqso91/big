一、Spark Streaming基础
	1、什么是Spark Streaming？
		Spark Streaming makes it easy to build scalable fault-tolerant streaming applications.
	   
	   特点：（1）易用，已经集成在了Spark中
	         （2）容错性：底层也是RDD
			 （3）支持Java、Scala、Python语言
			 
	2、演示Demo：执行WordCount单词计数
		使用消息服务器：netcat
	    执行Demo
		非常重要：一定要保证CPU的核数大于等于2
		
	3、开发自己的NetworkWordCount程序
		（*）问题：Spark Streaming 不记录之前状态

二、高级的特性
	1、什么是DStream？离散流
		（*）把连续的数据流变成不连续的RDD
		（*）本质上：Spark Streaming依然是离线计算，不是真正的流式计算
		
	2、重点介绍最后的两个算子
		（1）transform
		（2）updateStateByKey(func)：希望进行累加
			（*）如何记录之前的状态？
					需要设置一个检查点，把之前的结果保存到检查点目录下

	3、窗口操作：需要指定什么参数？？？？
		（*）参数是有要求的，不能随便设置
		错误
		The slide duration of windowed DStream (10000 ms) must be a multiple of 
		the slide duration of parent DStream (3000 ms)
		
		滑动的距离，必须是数据采样频率的整数倍
		
	4、集成Spark SQL：可以使用SQL语句来分析流式数据
	
	5、缓存和持久化：跟RDD一样
	6、支持检查点

三、数据源
	1、基本的数据源：文件流、RDD的队列流、套接字流（socketTextStream）
			讲义上，提供了一个工具，类似netcat消息服务器
	
	2、高级数据源
		（1）Flume
		（2）Apache Kafka：消息系统，类似Redis的消息
			（*）回顾：消息
				（*）消息类型：主题Topic：广播
				               队列Queue：一对一
				（*）消息系统类型：同步消息系统、异步消息系统
				（*）常见的消息产品：Redis
				                     Kafka
									 JMS
									 等等
									 
			（*）Apache Kafka的体系架构：只支持Topic的消息
			     核心的配置文件： conf/server.properties
		
四、性能优化的参数














